import ollama
from utils.save_helper import save_entry  # âœ… unified Smart Study logger

# Default model for Ollama
DEFAULT_MODEL = "llama3"


def ask_gpt(messages, model=DEFAULT_MODEL):
    """
    Send chat messages to Ollama (local Llama3 or other model)
    and log the interaction to the unified Smart Study timeline.
    """
    try:
        # ğŸ”¹ 1. Chat with Ollama
        response = ollama.chat(
            model=model,
            messages=messages
        )

        reply = response["message"]["content"]

        # ğŸ”¹ 2. Extract user query for context
        user_message = ""
        for m in reversed(messages):
            if m["role"] == "user":
                user_message = m["content"]
                break

        # ğŸ”¹ 3. Save chat to unified timeline
        try:
            save_entry(
                module="chatbot",
                title="Chatbot Interaction",
                content=f"User asked: {user_message[:120]}",
                metadata={
                    "model": model,
                    "user_message": user_message,
                    "assistant_reply": reply
                },
            )
        except Exception as log_err:
            print(f"âš ï¸ Failed to log chatbot message: {log_err}")

        # ğŸ”¹ 4. Return the AI response
        return reply

    except Exception as e:
        print("âŒ Llama call failed:", e)
        return "âš ï¸ Sorry, the local Llama model is not available. Please make sure Ollama is running and the model is pulled."
